{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as tf\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import functools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg19(pretrained=True).features[:-2]\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(4,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, activation = 'lrelu', norm = 'in'):\n",
    "        super(GatedConv2d, self).__init__()\n",
    "        self.pad = nn.ZeroPad2d(padding)\n",
    "        if norm is not None:\n",
    "            self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        else:\n",
    "            self.norm = None\n",
    "            \n",
    "        if activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace = True)\n",
    "        \n",
    "       \n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
    "        self.mask_conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        conv = self.conv2d(x)\n",
    "        mask = self.mask_conv2d(x)\n",
    "        gated_mask = self.sigmoid(mask)\n",
    "        x = conv * gated_mask\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class TransposeGatedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, norm=None, scale_factor = 2):\n",
    "        super(TransposeGatedConv2d, self).__init__()\n",
    "        # Initialize the conv scheme\n",
    "        self.scale_factor = scale_factor\n",
    "        self.gated_conv2d = GatedConv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, norm=norm)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor = self.scale_factor, mode = 'nearest')\n",
    "        x = self.gated_conv2d(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=4, latent_channels=64, out_channels=3):\n",
    "        super(GatedGenerator, self).__init__()\n",
    "        self.coarse = nn.Sequential(\n",
    "            # encoder\n",
    "            GatedConv2d(in_channels, latent_channels, 7, 1, 3, norm = None),\n",
    "            GatedConv2d(latent_channels, latent_channels * 2, 4, 2, 1),\n",
    "            GatedConv2d(latent_channels * 2, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 4, 2, 1),\n",
    "            # Bottleneck\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 2, dilation = 2),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 4, dilation = 4),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 8, dilation = 8),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 16, dilation = 16),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            # decoder\n",
    "            TransposeGatedConv2d(latent_channels * 4, latent_channels * 2, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 2, latent_channels * 2, 3, 1, 1),\n",
    "            TransposeGatedConv2d(latent_channels * 2, latent_channels, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels, out_channels, 7, 1, 3, activation = 'tanh', norm = None)\n",
    "        )\n",
    "        self.refinement = nn.Sequential(\n",
    "            # encoder\n",
    "            GatedConv2d(in_channels, latent_channels, 7, 1, 3, norm = None),\n",
    "            GatedConv2d(latent_channels, latent_channels * 2, 4, 2, 1),\n",
    "            GatedConv2d(latent_channels * 2, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 4, 2, 1),\n",
    "            # Bottleneck\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 2, dilation = 2),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 4, dilation = 4),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 8, dilation = 8),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 16, dilation = 16),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 4, latent_channels * 4, 3, 1, 1),\n",
    "            # decoder\n",
    "            TransposeGatedConv2d(latent_channels * 4, latent_channels * 2, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels * 2, latent_channels * 2, 3, 1, 1),\n",
    "            TransposeGatedConv2d(latent_channels * 2, latent_channels, 3, 1, 1),\n",
    "            GatedConv2d(latent_channels, out_channels, 7, 1, 3, activation = 'tanh', norm = None)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, mask):\n",
    "        # img: entire img\n",
    "        # mask: 1 for mask region; 0 for unmask region\n",
    "        # 1 - mask: unmask\n",
    "        # img * (1 - mask): ground truth unmask region\n",
    "        # Coarse\n",
    "     \n",
    "        first_masked_img = img * (1 - mask) + mask\n",
    "        first_in = torch.cat((first_masked_img, mask), 1)       # in: [B, 4, H, W]\n",
    "        first_out = self.coarse(first_in)                       # out: [B, 3, H, W]\n",
    "        # Refinement\n",
    "        second_masked_img = img * (1 - mask) + first_out * mask\n",
    "        second_in = torch.cat((second_masked_img, mask), 1)     # in: [B, 4, H, W]\n",
    "        second_out = self.refinement(second_in)                 # out: [B, 3, H, W]\n",
    "        return first_out, second_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, target_real_label=1.0, target_fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(input)\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InpaintDataset(data.Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.load_images()\n",
    "        \n",
    "    def load_images(self):\n",
    "        self.fns =[]\n",
    "        img_paths = sorted(os.listdir(self.img_dir))\n",
    "        for path in img_paths:\n",
    "            self.fns.append(os.path.join(self.img_dir, path))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.fns[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        \n",
    "        mask = self.random_ff_mask()\n",
    "        img = torch.from_numpy(img.astype(np.float32) / 255.0).permute(2, 0, 1).contiguous()\n",
    "        mask = torch.from_numpy(mask.astype(np.float32)).contiguous()\n",
    "        return img, mask\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        imgs = torch.stack([i[0] for i in batch])\n",
    "        masks = torch.stack([i[1] for i in batch])\n",
    "        return {\n",
    "            'imgs': imgs,\n",
    "            'masks': masks\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "    \n",
    "    def random_ff_mask(self, shape =256 , max_angle = 4, max_len = 40, max_width = 10, times = 15):\n",
    "            \"\"\"Generate a random free form mask with configuration.\n",
    "            Args:\n",
    "                config: Config should have configuration including IMG_SHAPES,\n",
    "                    VERTICAL_MARGIN, HEIGHT, HORIZONTAL_MARGIN, WIDTH.\n",
    "            Returns:\n",
    "                tuple: (top, left, height, width)\n",
    "            \"\"\"\n",
    "            height = shape\n",
    "            width = shape\n",
    "            mask = np.zeros((height, width), np.float32)\n",
    "            times = np.random.randint(times)\n",
    "            for i in range(times):\n",
    "                start_x = np.random.randint(width)\n",
    "                start_y = np.random.randint(height)\n",
    "                for j in range(1 + np.random.randint(5)):\n",
    "                    angle = 0.01 + np.random.randint(max_angle)\n",
    "                    if i % 2 == 0:\n",
    "                        angle = 2 * 3.1415926 - angle\n",
    "                    length = 10 + np.random.randint(max_len)\n",
    "                    brush_w = 5 + np.random.randint(max_width)\n",
    "                    end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
    "                    end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
    "                    cv2.line(mask, (start_y, start_x), (end_y, end_x), 1.0, brush_w)\n",
    "                    start_x, start_y = end_x, end_y\n",
    "            return mask.reshape((1, ) + mask.shape).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InpaintDataset(img_dir='datasets/places365standard_easyformat/places365_standard/train/waterfall')\n",
    "dataloader = data.DataLoader(dataset, batch_size=4, collate_fn = dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    imgs = batch['imgs']\n",
    "    masks = batch['masks']\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GANLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bdcc75eef256>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNLayerDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_sigmoid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_P\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptualNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcriterion_adv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGANLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcriterion_rec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcriterion_per\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GANLoss' is not defined"
     ]
    }
   ],
   "source": [
    "model_G = GatedGenerator()\n",
    "model_D = NLayerDiscriminator(3, use_sigmoid=True)\n",
    "model_P = PerceptualNet()\n",
    "criterion_adv = GANLoss()\n",
    "criterion_rec = nn.MSELoss()\n",
    "criterion_per = nn.L1Loss()\n",
    "optimizer_D = torch.optim.Adam(model_D.parameters(), lr=1e-4)\n",
    "optimizer_G = torch.optim.Adam(model_G.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'D': model_D.state_dict(),\n",
    "    'G': model_G.state_dict()\n",
    "}, 's.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_params(model_G))\n",
    "print(count_params(model_D))\n",
    "print(count_params(model_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_ff_mask(shape =256 , max_angle = 4, max_len = 40, max_width = 10, times = 15):\n",
    "            \"\"\"Generate a random free form mask with configuration.\n",
    "            Args:\n",
    "                config: Config should have configuration including IMG_SHAPES,\n",
    "                    VERTICAL_MARGIN, HEIGHT, HORIZONTAL_MARGIN, WIDTH.\n",
    "            Returns:\n",
    "                tuple: (top, left, height, width)\n",
    "            \"\"\"\n",
    "            height = shape\n",
    "            width = shape\n",
    "            mask = np.zeros((height, width), np.float32)\n",
    "            times = np.random.randint(times)\n",
    "            for i in range(times):\n",
    "                start_x = np.random.randint(width)\n",
    "                start_y = np.random.randint(height)\n",
    "                for j in range(1 + np.random.randint(5)):\n",
    "                    angle = 0.01 + np.random.randint(max_angle)\n",
    "                    if i % 2 == 0:\n",
    "                        angle = 2 * 3.1415926 - angle\n",
    "                    length = 10 + np.random.randint(max_len)\n",
    "                    brush_w = 5 + np.random.randint(max_width)\n",
    "                    end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
    "                    end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
    "                    cv2.line(mask, (start_y, start_x), (end_y, end_x), 1.0, brush_w)\n",
    "                    start_x, start_y = end_x, end_y\n",
    "            return mask.reshape((1, ) + mask.shape).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('datasets/places365standard_easyformat/places365_standard/train/waterfall/00000003.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (256, 256))\n",
    "img = torch.from_numpy(img.astype(np.float32) / 255.0).permute(2, 0, 1).contiguous()\n",
    "img_tensor = img.unsqueeze(0)\n",
    "mask = random_ff_mask()\n",
    "mask = torch.from_numpy(mask).contiguous().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(img):\n",
    "    np_img = img.squeeze(0).detach().cpu().numpy()\n",
    "    return np_img.transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(visualize(first_out_wholeimg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_out, second_out = model_G(img_tensor, mask)\n",
    "\n",
    "first_out_wholeimg = img_tensor * (1 - mask) + first_out * mask     \n",
    "second_out_wholeimg = img_tensor * (1 - mask) + second_out * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train discriminator\n",
    "optimizer_D.zero_grad()\n",
    "\n",
    "fake_D = model_D(second_out_wholeimg.detach())\n",
    "real_D = model_D(img_tensor)\n",
    "\n",
    "loss_fake_D = criterion_adv(fake_D, target_is_real=False)\n",
    "loss_real_D = criterion_adv(real_D, target_is_real=True)\n",
    "\n",
    "loss_D = (loss_fake_D + loss_real_D) *0.5\n",
    "\n",
    "loss_D.backward()\n",
    "optimizer_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Generator\n",
    "\n",
    "optimizer_G.zero_grad()\n",
    "\n",
    "fake_D = model_D(second_out_wholeimg)\n",
    "G_loss = criterion_adv(fake_D, target_is_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction loss\n",
    "\n",
    "loss_rec_1 = criterion_rec(first_out_wholeimg, img_tensor)\n",
    "loss_rec_2 = criterion_rec(second_out_wholeimg, img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptual loss\n",
    "\n",
    "img_featuremaps = model_P(img_tensor)                            # feature maps\n",
    "second_out_wholeimg_featuremaps = model_P(second_out_wholeimg)\n",
    "\n",
    "loss_P = criterion_per(second_out_wholeimg_featuremaps, img_featuremaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lambda_G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-dbfe5f51e2fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_G\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mG_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_rec_1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss_rec_1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_rec_2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss_rec_2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_per\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss_P\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lambda_G' is not defined"
     ]
    }
   ],
   "source": [
    "loss = lambda_G * G_loss + lambda_rec_1 * loss_rec_1 + lambda_rec_2 * loss_rec_2 + lambda_per * loss_P\n",
    "loss.backward()\n",
    "optimizer_G.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
